--- Page 1 ---
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 8, NO. 10, OCTOBER 2023
6835
LEMMA: Learning Language-Conditioned
Multi-Robot Manipulation
Ran Gong
, Xiaofeng Gao
, Qiaozi Gao
, Suhaila Shakiah
, Govind Thattai
,
and Gaurav S. Sukhatme
, Fellow, IEEE
Abstract—Complex manipulation tasks often require robots
with complementary capabilities to collaborate. We introduce a
benchmark for LanguagE-Conditioned Multi-robot MAnipulation
(LEMMA) focused on task allocation and long-horizon object ma-
nipulation based on human language instructions in a tabletop
setting. LEMMA features 8 types of procedurally generated tasks
with varying degree of complexity, some of which require the
robots to use tools and pass tools to each other. For each task,
we provide 800 expert demonstrations and human instructions
for training and evaluations. LEMMA poses greater challenges com-
pared to existing benchmarks, as it requires the system to identify
each manipulator’s limitations and assign sub-tasks accordingly
while also handling strong temporal dependencies in each task.
To address these challenges, we propose a modular hierarchical
planning approach as a baseline. Our results highlight the potential
of LEMMA for developing future language-conditioned multi-robot
systems.
Index Terms—Data Sets for Robot Learning, Natural Dialog for
HRI, Multi-Robot Systems.
I. INTRODUCTION
T
HEREisgrowinginterestinconnectinghumanlanguageto
robot actions, particularly in single-agent systems [1], [2],
[3], [4], [5]. However, there remains a research gap in enabling
multi-robot systems to work together in response to language
input.
Recent vision and language tasks have primarily focused
on navigation and object interactions [4], [6], [7]. However,
the lack of physical manipulation in these works makes the
settings oversimpliﬁed. Although some recent studies, such
as [1], [5], address vision and language object manipulation in
single-robot settings, the language instructions provided spec-
ify only short-term goals, neglecting long-term objectives. [8]
attempt to address these limitations by exploring long-horizon
planning with manipulation for individual robots. Nevertheless,
there remains a need to investigate multi-robot systems capable
Manuscript received 18 April 2023; accepted 21 August 2023. Date of
publication 7 September 2023; date of current version 15 September 2023. This
letter was recommended for publication by Associate Editor L. Rozo and Editor
A. Faust upon evaluation of the reviewers’ comments. This work was supported
by Amazon Alexa AI. (Corresponding author: Xiaofeng Gao.)
Ran Gong is with the Center for Vision, Cognition, Learning, and Autonomy,
UCLA, Los Angeles, CA 90025 USA (e-mail: nikepupu@ucla.edu).
Xiaofeng Gao, Qiaozi Gao, Suhaila Shakiah, and Govind Thattai are with
the Amazon Alexa AI, San Jose, CA 95129 USA (e-mail: xfgao@g.ucla.edu;
qiaozikl@gmail.com; ssshakia@amazon.com; gowin.thattai@gmail.com).
Gaurav S. Sukhatme is with the Amazon Alexa AI, San Jose, CA 95129
USA, and also with the Department of Computer Science, USC Viterbi School
of Engineering, Los Angeles, CA 90089 USA (e-mail: gaurav@usc.edu).
Project website: https://lemma-benchmark.github.io
Digital Object Identiﬁer 10.1109/LRA.2023.3313058
of accomplishing a broader range of long-horizon tasks while
following language instructions.
Learning policies for multi-robot systems introduces distinct
challenges, including diverse capabilities arising from physical
constraints such as the location and reach of different robots.
Moreover,taskplanningheavilydependsonthespatialandphys-
ical relations between the objects and robots, in addition to the
geometries of the objects. To ensure suitable task assignments,
an awareness of each robot’s speciﬁc physical capabilities is
needed.
To tackle the language-conditioned vision-based multi-robot
object manipulation problem, we have developed LEMMA,
a benchmark that contains 8 types of collaborative object
manipulation tasks with varying degrees of complexity. Some
tasks require the robot to use tools for object-object interactions.
For each task, the object poses, appearances, and robot types are
randomized, requiring object affordance estimation and robot
capability understanding. To enable multi-task learning, each
task is paired with an expert demonstration and several language
instructions specifying the task at different granularities. As
a result, LEMMA introduces a diverse range of challenges
in multi-robot collaboration, including physics-based object
manipulation, long-horizon task planning, scheduling and
allocation, robot capability and object affordance estimation,
tool use, and language grounding. Each aspect poses distinct
challenges and is crucial for a multi-robot system that follows
human instructions to complete tasks. To evaluate existing
techniques on LEMMA, we further provide several baseline
methods and compare their performance to each other. We assess
task performance by utilizing the latest language-conditioned
policy learning models. Our results indicate that current models
for language-conditioned manipulation and task planning face
signiﬁcant challenges in LEMMA, especially when dealing with
complex human instructions.
We make the following contributions:
r We design eight novel collaborative object manipulation
tasks involving robots with different physical conﬁgura-
tions implemented in Nvidia Omniverse - Isaac Sim.
r We provide an open-source dataset comprising 6,400 ex-
pert demonstrations and natural language instructions, in-
cluding human and high-level instructions.
r Weimplementamodularhierarchicalplanningapproachas
a baseline, which integrates language understanding, task
planning, task allocation, and object manipulation.
II. RELATED WORK
Language Conditioned Manipulation: Recent research has
shown a growing interest in connecting human language to
robot actions [2], [3], [5], [6], [9], [10], [11], [12]. However,
2377-3766 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 2 ---
6836
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 8, NO. 10, OCTOBER 2023
TABLE I
COMPARISON WITH OTHER BENCHMARKS.
these studies typically use a single-robot setting. In contrast, our
work emphasizes multi-robot collaboration. In particular, our
task settings are designed to require the collaboration of two
robots to successfully complete the task. In contrast, in [13],
[14], a single agent can still ﬁnish the task although multiple
agents are available. In our setting, due to the limited workspace
reachability of each robot, each target object can only be manip-
ulated by one robot initially, making collaboration necessary to
achieve the task goals.
Visual Multi-Agent Collaboration: Visual multi-agent col-
laboration has attracted attention in recent embodied AI re-
search[13],[14],[15],[16],[17],[18],[19].However,amongthe
works that involve object manipulation, simpliﬁed non-physics
based atomic actions are often employed as an abstraction for
manipulation. These works often use a magic glove to attach
an object to the gripper as long as hand-crafted conditions
are met. For example, an object within 15 cm to the gripper
can be automatically snapped to the gripper [20]. While this
simpliﬁcation does not affect learning robot task planning, it
is unsuitable for learning low-level manipulation policies. We
do not make such a simpliﬁcation here instead using a gripper
to interact with objects physically. Additionally, most previous
works study the collaboration problem in a single-task set-
ting. In contrast, our work uses a multi-task setting requiring
the comprehension of a textual description to understand the
goal.
Bimanual Robot Manipulation: There is a rich set of literature
on bimanual robot manipulation [24], [25], [26], [27], [28],
[29], [30]. These works address important problems in dual-arm
coordination with a focus on coordinated control and collision
avoidance. However, there is less exploration of multi-robot
task planning and allocation for long-horizon tasks with strong
temporal dependencies, along with workspace management.
In addition, these works typically do not involve vision and
language inputs, especially for the recognition of the physical
limitations of different robots from vision input. More impor-
tantly, previous research usually employs robot arms of the
same type. In contrast, our work considers the settings of both
heterogeneous and homogeneous robot arms.
Visual Robot Task and Motion Planning: Traditionally, most
works in this area use search over pre-deﬁned domains for
planning, which require extensive domain knowledge and ac-
curate perception. Moreover, they often scale poorly with an
increasing number of objects. Another line of work involves
generating task and motion plans given scene images [31], [32].
In contrast, in our settings, the model uses both RGBD images
and textual descriptions as input for multi-task learning. Most
recently, [33] generated robot policies in the form of code tokens
using large language models (LLMs). Nonetheless, they only
focus on single-agent task planning. We compare and contrast
our benchmark with existing works in Table I.
III. PROBLEM FORMULATION
Assume a robot system comprised of N robots that is tasked
to complete a complex manipulation task, the goal of which is
speciﬁed by a language instruction xL = {xl}L
l=1, which is a
sequence of L word tokens. The full task can be decomposed
into M sub-tasks dM = {dm}M
m=1. dM represents the full task,
and dm represent each sub-task in dM. Given the language
instruction xL, our goal is to ﬁnd a valid and optimal sub-task
allocation. We deﬁne qim and cim as the quality and cost,
respectively, for allocating robot i to work on sub-task m. Then
the combined utility for the sub-task is:
uim =

qim −cim,
if robot i can execute sub-task m
−∞.
otherwise
We deﬁne the assignment of sub-task m to robot i as
vim =

1,
robot i is assigned to sub-task m
0.
otherwise
with γm = i being an assignment variable for each sub-task m,
indicating that sub-task m is assigned to robot i.
The goal is to maximize the utility of the full manipulation
task under a time constraint. Deﬁning the execution time for task
m by robot i as τim, and the maximum time allowed to execute
the task as Tmax, we can express the task decomposition and
assignment problem as follows:
arg max
v
N

i=1
M

m=1
uimvim
(1)
Subject to:

i

m
τimvim
≤Tmax

i
vim
≤1
∀m ∈M
vim
∈{0, 1}
∀i ∈N, ∀m ∈M
As pointed out by [34], this problem cannot be solved in polyno-
mial time. In this work, we tackle this problem by learning from
expert demonstrations, so that each sub-task can be assigned
to a capable robot to ensure successful task execution. With
the sub-task dm and its assignment γm, an object manipula-
tion policy π(st+1|st, ot
N, xL, dm, γm) can be used to move a
speciﬁc object from its current pose st to its target pose st+1,
given the observations ot
N = {ot
i}N
i=1 and language instruction
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 3 ---
GONG et al.: LEMMA: LEARNING LANGUAGE-CONDITIONED MULTI-ROBOT MANIPULATION
6837
Fig. 1.
Expert demonstrations and high-level instructions of tasks in LEMMA . We use minimal instructions that specify the goal, thus, it is possible that two
different tasks may have the same instruction. E.g., tasks in (c) and (d) have the same instruction, but (d) requires robots to make use of the tool to poke the blocks
so that the red robot can reach them. Note that the pair of robots involved in each demonstration can be different. The pair of robots in homogeneous settings (e.g.,
a, b, and d) have the same reach, while in heterogenous cases the reach can be different for each robot (e.g., c).
xL. In this work, the observation ON consists of robot joint
conﬁgurations, RGBD images associated with each robot, and
camera parameters.
IV. LEMMA BENCHMARK
We introduce LEMMA to address the language-conditioned
multi-robot manipulation problem. This benchmark is designed
to evaluate a system’s ability to dynamically perform task alloca-
tion and object manipulation in a tabletop environment. LEMMA
sets itself apart from existing language-conditioned robotic ma-
nipulation benchmarks, such as [2], [3], [22], in several key
aspects:
r All tasks in LEMMA feature strong temporal dependencies,
making the execution order of sub-tasks critically impor-
tant. Out-of-order execution will result in task failure.
r LEMMA tasks are exclusively multi-agent based. Due
to the robots’ reachable space limitations, it is im-
possible to complete tasks in LEMMA using a single
agent.
r As illustrated in Fig. 1, robots are provided with only
minimal high-level instructions. This requires the model to
have a deep understanding of the environment and plan a
sequence of actions accordingly to reach the goal speciﬁed
by the instruction. Using a language classiﬁer to determine
the task type and a template to form task plans, as in [35], is
inadequate, as multiple tasks may share the same language
instructions.
r LEMMA allows robots with different physical conﬁgura-
tions to collaborate on manipulation tasks. Two types of
robots are provided in LEMMA, namely UR10 and UR5.
A detailed comparison between LEMMA and other related
benchmarks is shown in Table I.
A. Task Settings
In LEMMA, we focus on tasks that require a multi-robot system
to complete, given a single instruction and visual observations
from top-down cameras placed above each robot. Speciﬁcally,
we consider a two-robot setting under centralized control. The
high-level goal is speciﬁed by a single natural language instruc-
tion such as “Place the red block on top of the white pad”.
However, this instruction may not provide all the necessary
details on how to complete the task. To thoroughly assess system
performance in language-conditioned multi-robot collaboration,
we have designed 8 tasks of varying difﬁculty. The tasks are
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 4 ---
6838
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 8, NO. 10, OCTOBER 2023
TABLE II
EIGHT TYPES OF TASKS IN LEMMA
implemented in a simulated tabletop environment in NVIDIA
Omniverse Isaac-Sim. Task statistics can be found in Table II.
Pass: The ﬁrst robot is required to pick up a cube of a
designated color in its own reachable workspace and place it
within a shared workspace, i.e. a space that is reachable by
both robots. Following this, the second robot must pick up the
designated cube and position it on top of a speciﬁed pad in its
reachable workspace.
Stack: The ﬁrst robot picks up a designated cube in its
reachable workspace and places it in the shared workspace.
Subsequently, the second robot picks up a different designated
cube and stacks it on top of the ﬁrst cube.
Poke: Initially, the cube is unreachable by either robot. One
robot has access to a tool and must use it to poke the designated
cube into the other robot’s reachable workspace. The second
robot then picks up the cube and places it in the target pad in its
reachable workspace.
Hook: Initially, the cube is unreachable by either robot. How-
ever, one of the robots can use a tool to hook the cube into its
own reachable space. After that, the robot need to move it to
the shared workspace so that the other robot can pick it up and
position it on the target pad.
Pass2: As an extension of Pass, the task requires the robots to
pass two different objects to each other and place them in their
respective target locations.
Stack2: As an extension of Stack, the task requires the robots
to construct two block towers, each requires two cubes of dif-
ferent designated colors.
Poke&Stack: The task requires one robot to use a tool to poke
two designated cubes into the other robot’s workspace. The other
robot then places one cube on top of the other.
Hook&Stack: The task requires one robot to use a tool
to hook a designated cube into its own reachable space and
pass the tool to the second robot, allowing it to perform
its hook action on the second cube. The ﬁrst robot then
moves the ﬁrst hooked cube to the shared workspace so that
the second robot can pick it up and stack it on the second
block.
B. Action Space and Observation Space
The default action space of each robot is the end-effector
position. Nevertheless, alternative control mechanisms such as
joint positions, joint velocities, and joint torques can also be
accommodated for controlling 6 degrees-of-freedom robots.
Due to physical constraints, each robot has a unique reachable
workspace, allowing it to only interact with a limited set of
objects in the task.
To obtain visual observations, we place a ﬁxed RGBD camera
above each robot. Each camera has a limited ﬁeld of view and
cannot capture all the objects. We follow [1] to process the vision
input: we ﬁrst generate the scene point cloud based on all RGBD
images and camera parameters, and use the point cloud to obtain
the top-down orthographic RGBD reconstruction of the scene.
C. Task Generation
For diversity among the tasks, we use a rejection sampling
mechanism to generate task instances. Each task instance spec-
iﬁes the initial environment conﬁgurations and goal conditions.
To ensure that the task completion requires both robots, we make
sure that the initial location of the target object falls into the
reachable workspace of only one robot. In addition to the target
object, we also add some distractor objects to make the task
more challenging:
1) Specify each robot’s type, location, and color. There
are two robot types, UR5 and UR10, respectively.
Each robot has two different colors, red and white.
Colors and types are randomly assigned to each
robot.
2) Sample the goal condition of the task, including the colors
of the target objects and their goal locations, according
to the task type. E.g., the goal condition for the Stack
pink on red task is satisﬁed when a pink block is on
top of a red block, as shown in Fig. 1(c). There are ﬁve
available colors for blocks and pads, including pink, red,
white, blue, and green. The color of the tool is always
yellow.
3) Sample the initial locations of target objects, so that each
object is only within the reachable space of a single robot.
4) Sample the colors and locations of distractor objects while
making sure the colors of the target objects are unique.
The above sampling mechanism is repeated until one valid
task instance is generated.
D. Expert Demonstration
Given a task speciﬁcation, we use an oracle task and motion
planner to create expert demonstrations. Based on the initial
conﬁgurations and goal conditions of the task, the oracle creates
a task plan consisting of a sequence of sub-tasks dM, and the
allocated robot γm for each sub-task dm. The sub-task follows
a pick-and-place procedure, specifying the target object to pick
up and the target pose at which to place the object. Once the
task plan is generated, an RMPﬂow motion planner is utilized
to generate a motion plan based on ground truth object locations
at each time step. All the motion plans are executed by the
designated robot, and the corresponding RGBD images and
camera poses are recorded to form the expert demonstration
data.
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 5 ---
GONG et al.: LEMMA: LEARNING LANGUAGE-CONDITIONED MULTI-ROBOT MANIPULATION
6839
TABLE III
EXAMPLE HIGH-LEVEL INSTRUCTION AND CROWD-SOURCED HUMAN LANGUAGE INSTRUCTION TEMPLATES TO SPECIFY MANIPULATION TASKS IN LEMMA
E. Language Instructions
We assign high-level instruction and a human instruction to
each task instance. For high-level instruction, we manually de-
ﬁne a template for each task and lexicalize the template using the
goal of the task. For human instruction, we ﬁrst crowd-sourced
500templatesonAmazonMechanicalTurkandselected80valid
templates that are not too verbose and uniquely specify the goal
given the visual observation, 10 for each task type. For each task
instance, we then sample a template from the template pool and
lexicalize it to generate human instruction. Some examples of
the instruction templates can be found in Table III.
In summary, each datapoint in LEMMA contains the following
information: 1) a manipulation task, speciﬁed by the initial
conﬁgurations and goal conditions, 2) an expert demonstration,
including camera poses and RGBD images at each timestep, 3)
a high-level instruction and a human instruction specifying the
task. As a result, we generate 800 data sessions for each task
type, with a total of 6400 sessions. For each task type, we keep
700 sessions in the training set, 40 in the validation set, and
60 in the test set.
F. Evaluation Metrics
We have adopted success rate as the evaluation metric in
LEMMA . Task success is deﬁned as 1 if the task goal-conditions
are met at the end of the episode, and 0 otherwise. The time
constraint of each episode Tmax is set to 100 seconds. The task
speciﬁed by the instruction Place the red block on the green
circle, for example, is considered successful if, at the end of the
episode, the red block is on top of the green circular pad.
V. BASELINE MODELS
Consider the problem of learning multi-agent task and motion
planning with two robots given a single language instruction and
visual observations. The problem can be decomposed into two
sub-problems: (a) multi-robot task planning and task allocation,
and(b) single-robot planninggiventheassignedsub-task. Tothis
end, we design a modular baseline model which includes a high-
level planner for deciding which sub-task a robot should work on
and a low-level planner for generating pick and place locations
for the gripper given the assigned sub-task. Fig. 2(a) shows the
overall architecture of our baseline model. We follow [1] to use
the fused point clouds generated from the RGB and depth images
of two cameras. Then we use the top orthographic projection of
the point cloud as the visual input.
A. Action Primitives
We deﬁne six action primitives: move, prehook, hook, pre-
poke, poke and stop. The non-stop action primitives follow
generalized pick-and-place settings. Move is the standard pick-
and-place primitive, requiring the robot to pick up the object and
move it to a speciﬁed location. Prehook and prepoke require the
robot to pick up the tool and align it with the target object to
prepare for hook and poke respectively. The required height of
the gripper is different for each primitive. For hook and poke, the
height of the gripper after picking is set at 5 cm, enabling the tool
to come into contact with the target object. For other primitives,
the height of the gripper is set to 30 cm to avoid contact with
other objects between the pick and place actions. The sequence
of action primitives required to complete the task depends on the
relative poses of target objects, such as cubes, tools, and pads.
As a result, the same language instruction can correspond to
completely different sequences of primitive actions, depending
on the speciﬁc arrangement of these objects.
B. Multi-Robot Task Planning and Task Allocations
The multi-robot collaboration task can be decomposed into
a sequence of sub-tasks, each can be completed by a single
robot. The sub-task allocation can be represented by a tuple
(dt
m, gt
m, γt
m). gt
m = (e, p, q) represents the entities to specify
the sub-task dt
m, including the action primitive e, the pick entity
p and place entity q. E.g. (Move, Red Cube, Shared Space) indi-
cates moving a red cube on top of the shared workspace between
two robots. γt
m is the sub-task assignment indicating which robot
is to perform the task. (dt
m, gt
m, γt
m) are further lexicalized using
templates to form the sub-instruction specifying the sub-task and
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 6 ---
6840
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 8, NO. 10, OCTOBER 2023
Fig. 2.
Our baseline model involves a high-level task planning module and a low-level planning module. The high-level planning module takes as input the
top-down fused scene image and human instruction to generate sub-instructions that assign the corresponding sub-tasks to robots based on their limitations. Each
sub-task is speciﬁed by action primitives (in orange) and objects (in blue). The low-level planning module uses the sub-instruction and top-down projection of the
scene to generate pick and place locations (visualized as purple and green stars respectively) of the gripper in the scene.
its allocation (Fig. 2(a)). In practice, we use Episodic Trans-
former [36], a vision and language task planner to generate the
sub-instructions. The approach is a language-conditioned visual
task planning method that employs a transformer architecture
for long-horizon planning. As shown in Fig. 2(b), ET uses the
historical visual and language information in the entire episode
to capture long-term dependencies between actions. It leverages
the transformer architecture to ﬁrst separately encodes image
histories, language instructions, and past action histories, and
then perform cross-attention across modalities to decode robot
assignment, action primitives, and the target object separately.
The choice between neural-based open-loop planning and
closed-loop planning is often debatable. Open-loop planning
cannot adapt to errors made in the planning process. However,
it is more stable to train since the training distribution is often
more aligned with the testing distribution. Here we consider both
open-loop planning and close-loop planning for our benchmark
and compare their performance. Note the original Episodic
Transformer is closed-loop only and requires new observation
at each time step to plan the next action (i.e. Single-step). We
modify the algorithm to use only the initial visual observation
by providing it as input repeatedly during the loop to plan the
whole sub-instruction sequences (i.e. Multi-step).
C. Single Agent Object Manipulation and Grounding
In this work, we use CLIPort [1] as the low-level planner.
Formally, at each time step, the algorithm focuses on learning a
goal-conditioned policy π that produces actions at based on
the current visual observation ot and a language instruction
xt
L = {xt
l}L
l=1. The visual observation ot is an orthographic
top-down RGB-D reconstruction of the scene, where each pixel
corresponds to a point in 3D space. As shown in Fig. 2(a), in
our use case, each input language instruction xt
L = (gt
m, γt
m)
speciﬁes a sub-task being allocated to robot γt
m at time step t.
As a result, the goal-conditioned policy is deﬁned as
π

ot
N, xt
L

= π

ot
N, gt
m, γt
m

→at = (Tpick , Tplace ) ∈A,
where the actions a = (T pick, T place) denote the end-effector
poses for picking and placing respectively. CLIPort is designed
for tabletop tasks, with T pick, T place ∈SE(2).
Fig. 3.
Multi-Agent Cliport Model architecture. The output is extended to
include robot assignment and action primitive for each predicted action.
D. Multi-Agent Cliport
Since the sub-instruction already contains the sub-task al-
location γt
m and action primitive e, the CLIPort module used
as our low-level planner only needs to predict the pick and
place location. To compare with this modular approach, we
present a modiﬁed version of the original CLIPort module
(i.e. M-CLIPort) to perform task planning and task allocation
explicitly in an end-to-end fashion. We extend the output to a
higher dimensional vector to predict the robot assignment and
action primitive to use in addition to pick and place locations,
as shown in Fig. 3.
VI. EXPERIMENTS
A. Evaluation Workﬂow
We train the CLIPort module for 300 K steps on the training
set and save a checkpoint every 20 K steps. Then we per-
form checkpoint selection on the validation split for both the
high-level planner and the low-level planner. For the low-level
planner, we use the ground-truth sub-instructions as input for
checkpoint selection. For the high-level planner, we train the
EpisodicTransformermodelfor30epochsandsaveacheckpoint
at the end of each epoch. We choose the best checkpoint based
on the accuracy of predicted task plans on the validation split.
We report the performance of a combination of best-performing
high-level and low-level checkpoints on the test set. Since the
physics and rendering in Isaac-sim are not deterministic, we
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 7 ---
GONG et al.: LEMMA: LEARNING LANGUAGE-CONDITIONED MULTI-ROBOT MANIPULATION
6841
TABLE IV
PERFORMANCE ON THE TEST SET WITH HIGH-LEVEL INSTRUCTIONS
TABLE V
PERFORMANCE ON THE TEST SET WITH HUMAN INSTRUCTIONS
evaluate all tasks for 10 runs and report the means and standard
deviations.
B. Experiment Results
1) High-Level Instructions: The results shown in Table IV
compare language-conditioned policies with different task-
planning modules. The M-CLIPort fails for all tasks with longer
horizons. In comparison, our modular hierarchical planning
approach works reasonably well for most tasks but fails for
very long-horizon tasks (i.e. poke&stack and hook&stack). As
an ablation, we further supply ground truth task allocation to
the planning module (i.e. GTA). The results show that system
performance can be greatly improved with ground truth task
allocation. This indicates that task allocation is quite challeng-
ing since it requires understanding the reachable workspace of
robots to determine which robot should be assigned to a certain
sub-task.
2) Human Instructions: The results shown in Table V
demonstrate the system performance under human instructions.
Human instructions are more complex than high-level instruc-
tions since they feature different input lengths, levels of detail,
and word choices. Our results show there is a signiﬁcant gap
between the performance of high-level instructions and human
instructions, showing that current models are not capable of
handling the increased complexity in language. Among all the
results, models perform signiﬁcantly worse on Stack2 and Pass2
with human instructions. This discrepancy is likely due to the
fact that the orders of objects vary in human instructions for these
tasks (e.g. Place the red cube under the white cube” vs “Place
the white cube on top of the red cube”), as demonstrated in Table
III, which poses greater challenges in language understanding.
In addition, for tasks requiring tool use, human instructions do
not always involve the tools, e.g. one instruction for poke is
“push the red block toward the other robot and put it on the blue
circle”. In these cases, the model often fails to predict the correct
object to manipulate.
3) Further Analysis: To provide more insight into the fac-
tors affecting task performance, we further show a breakdown
of performance under different settings, including robot types
TABLE VI
IMPACT OF DISTRACTORS (SINGLE-STEP PLANNING)
TABLE VII
IMPACT OF ROBOT TYPES
and the number of distractors in the scene. We observe that
in general, the task performance decreases with an increas-
ing number of distractor objects (Table VI). As for the robot
types, the collaborations among two UR10 s exhibit signiﬁcantly
higher performance using the hierarchical planning model given
high-level instructions (Table VII). This is probably due to
the fact that UR5s have a smaller reachable space compared
to UR10 s, making it more critical to accurately predict the
reachable workspace of each robot.
VII. CONCLUSION, LIMITATIONS AND FUTURE WORK
In this letter, we presented LEMMA, the ﬁrst public benchmark
for language-conditioned multi-robot tabletop manipulation.
LEMMA combines the problems of language grounding, task
planning, task allocation, tool use, capability estimation, long-
horizon manipulation, and multi-modal scene understanding.
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 8 ---
6842
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 8, NO. 10, OCTOBER 2023
All these subproblems of LEMMA pose signiﬁcant challenges
for existing algorithms. We plan to open-source the simulation
environment, the generated dataset, the baseline models, and
other tools used during the project’s development stage, and we
hope our benchmark can inspire new methods in these areas.
In this work, we assume tasks can be easily decomposed into
independent sub-goals and we restrict the object manipulation
problem to SE2. Extending it to SE3 with a more diverse set of
tasks and objects will be an important future direction. In our
setting robots have different capabilities if they have different
reachable spaces. Of course, robots can differ in multiple other
ways such as payload, gripper types, etc. We use instructions
generated by templates, which are not as diverse as human
natural language instructions. Leveraging LLMs to produce
more diverse instructions can be a promising way to increase
linguistic diversity. This work did not speciﬁcally focus on
bimanual robot control, and one important future direction is
to explore challenges inherent to bimanual control including
dual-arm coordination and collision avoidance with real robot
experiments. However, this study is the ﬁrst of its kind to
study vision-based language-conditioned multi-robot collabora-
tion and our simpliﬁcations serve as a reasonable starting point
for research in this area.
REFERENCES
[1] M.Shridhar,L.Manuelli,andD.Fox,“CLIPort:Whatandwherepathways
for robotic manipulation,” in Proc. 5th Conf. Robot Learn., 2022, pp. 894–
906.
[2] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, “CALVIN: A
benchmark for language-conditioned policy learning for long-horizon
robot manipulation tasks,” IEEE Robot. Automat. Lett., vol. 7, no. 3,
pp. 7327–7334, Jul. 2022.
[3] K. Zheng, X. Chen, O. C. Jenkins, and X. E. Wang, “VLM-
BENCH: A compositional benchmark for vision-and-language manipu-
lation,” in Proc. Neural Inf. Process. Syst. Track Datasets Benchmarks,
2022, 665–678.
[4] P. Anderson et al., “Vision-and-language navigation: Interpreting visually-
grounded navigation instructions in real environments,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3674–3683.
[5] M. Shridhar, L. Manuelli, and D. Fox, “Perceiver-actor: A multi-task
transformer for robotic manipulation,” in Proc. Conf. Robot Learn., 2023,
pp. 785–799.
[6] M. Shridhar et al., “ALFRED: A benchmark for interpreting grounded
instructions for everyday tasks,” in Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit., 2020, pp. 10740–10749.
[7] X. Gao, Q. Gao, R. Gong, K. Lin, G. Thattai, and G. S. Sukhatme,
“DialFRED: Dialogue-enabled agents for embodied instruction follow-
ing,” IEEE Robot. Automat. Lett., vol. 7, no. 4, pp. 10049–10056,
Oct. 2022.
[8] W. Huanget al., “Inner monologue: Embodied reasoning through planning
with language models,” in Proc. Conf. Robot Learn., 2023, pp. 1769–1782.
[9] C. Lynch and P. Sermanet, “Language conditioned imitation learning over
unstructured data,” in Proc. Robot.: Sci. Syst., 2021, pp. 1–18.
[10] A. Zenget al., “Socratic models: Composing zero-shot multimodal reason-
ing with language,” in Proc. 11th Int. Conf. Learn. Representations, 2022,
pp. 1–35.
[11] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. B. Amor,
“Language-conditioned imitation learning for robot manipulation tasks,”
in Proc. Adv. Neural Inf. Process. Syst., 2020, pp. 13139–13150.
[12] S. Nairet al., “Learning language-conditioned robot behavior from ofﬂine
data and crowd-sourced annotation,” in Proc. Conf. Robot Learn., 2022,
pp. 1303–1315.
[13] S. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun, “Multi-agent embodied
question answering in interactive environments,” in Proc. 16th Eur. Conf.
Comput. Vis., 2020, pp. 663–678.
[14] X. Liu, X. Li, D. Guo, S. Tan, H. Liu, and F. Sun, “Embodied multi-agent
task planning from ambiguous instruction,” in Proc. Robot.: Sci. Syst.,
2022, pp. 1–14.
[15] U. Jain et al., “A cordial sync: Going beyond marginal policies for
multi-agent embodied tasks,” in Proc. 16th Eur. Conf. Comput. Vis., 2020,
pp. 471–490.
[16] U. Jain et al., “Two body problem: Collaborative visual task completion,”
in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 6689–
6699.
[17] H. Wang, W. Wang, X. Zhu, J. Dai, and L. Wang, “Collaborative visual
navigation,” 2021, arXiv:2107.01151.
[18] B. Chen, S. Song, H. Lipson, and C. Vondrick, “Visual hide and seek,” in
Proc. Artif. Life Conf., 2020, pp. 645–655.
[19] X. Liu, D. Guo, H. Liu, and F. Sun, “Multi-agent embodied visual semantic
navigation with scene prior knowledge,” IEEE Robot. Automat. Lett.,
vol. 7, no. 2, pp. 3154–3161, Apr. 2022.
[20] A. Szotet al., “Habitat 2.0: Training home assistants to rearrange their
habitat,” in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 251–266.
[21] Y. Deng et al., “MQA: Answering the question via robotic manipulation,”
in Proc. Robot.: Sci. Syst., 2020, pp. 1–10.
[22] A. Zenget al., “Transporter networks: Rearranging the visual world for
robotic manipulation,” in Proc. Conf. Robot. Learn., 2021, pp. 726–747.
[23] V. Sharma, P. Goyal, K. Lin, G. Thattai, Q. Gao, and G. S. Sukhatme,
“CH-MARL: A multimodal benchmark for cooperative, heterogeneous
multi-agent reinforcement learning,” 2022, arXiv:2208.13626.
[24] Y. Chen et al., “Towards human-level bimanual dexterous manipulation
with reinforcement learning,” in Proc. Adv. Neural Inf. Process. Syst.,
2022, pp. 5150–5163.
[25] K. Takata, T. Kiyokawa, I. G. Ramirez-Alpizar, N. Yamanobe, W. Wan,
and K. Harada, “Efﬁcient task/motion planning for a dual-arm robot from
language instructions and cooking images,” in Proc. IEEE/RSJ Int. Conf.
Intell. Robots Syst., 2022, pp. 12058–12065.
[26] S. Stavridis, P. Falco, and Z. Doulgeri, “Pick-and-place in dynamic envi-
ronments with a mobile dual-arm robot equipped with distributed distance
sensors,” in Proc. IEEE-RAS 20th Int. Conf. Humanoid Robots, 2021,
pp. 76–82.
[27] C. Smith et al., “Dual arm manipulation–A survey,” Robot. Auton. Syst.,
vol. 60, no. 10, pp. 1340–1353, 2012.
[28] H. Zhang, P.-J. Lai, S. Paul, S. Kothawade, and S. Nikolaidis, “Learning
collaborative action plans from youtube videos,” in Proc. Int. Symp. Robot.
Res., 2019, pp. 208–223.
[29] S. Stepputtis, M. Bandari, S. Schaal, and H. B. Amor, “A system for
imitation learning of contact-rich bimanual manipulation policies,” in
Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2022, pp. 11810–11817.
[30] P. Lertkultanon and Q.-C. Pham, “A certiﬁed-complete bimanual ma-
nipulation planner,” IEEE Trans. Automat. Sci. Eng., vol. 15, no. 3,
pp. 1355–1368, Jul. 2018.
[31] D. Driess, J.-S. Ha, and M. Toussaint, “Deep visual reasoning: Learning
to predict action sequences for task and motion planning from an initial
scene image,” in Proc. Robot: Sci. Syst. Found., 2020, pp. 1–10.
[32] D. Driess, J.-S. Ha, and M. Toussaint, “Learning to solve sequential
physical reasoning problems from a scene image,” Int. J. Robot. Res.,
vol. 40, no. 12-14, pp. 1435–1466, 2021.
[33] I. Singh et al., “ProgPrompt: Generating situated robot task plans using
large language models,” in Proc. IEEE Int. Conf. Robot. Automat., 2023,
pp. 11523–11530.
[34] G. A. Korsah, A. Stentz, and M. B. Dias, “A comprehensive taxonomy
for multi-robot task allocation,” Int. J. Robot. Res., vol. 32, no. 12,
pp. 1495–1512, 2013.
[35] S. Y. Min, D. S. Chaplot, P. K. Ravikumar, Y. Bisk, and R. Salakhutdinov,
“FILM: Following instructions in language with modular methods,” in
Proc. Int. Conf. Learn. Representations, 2021, pp. 1–17.
[36] A. Pashevich, C. Schmid, and C. Sun, “Episodic transformer for vision-
and-language navigation,” in Proc. IEEE/CVF Int. Conf. Comput. Vis.,
2021, pp. 15942–15952.
Authorized licensed use limited to: Consortium - Saudi Arabia SDL. Downloaded on February 16,2026 at 10:10:32 UTC from IEEE Xplore.  Restrictions apply. 
